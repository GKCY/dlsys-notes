## 大模型训练？

- 大模型训练的最主要瓶颈是显存不够，一张卡放不下。显存的来源有权重值、中间激活值以及优化器状态。
- 并行可以加速

## 数据并行（DP）

数据并行在每张卡上都存储相同的模型有着相同的权重，在每个minibatch的最后，计算完loss后，每张卡同步梯度或权重。有两种同步方式：

- Bulk synchronous parallels (BSP):每张卡在每个minibatch的最后同步数据，这个时候所有卡都必须等待同步结束。
- Asynchronous parallel (ASP): 每张卡异步的更新数据，不需要等待或暂停。这个时候有可能会获取到过时的权重。这种方式虽然会加快处理数据的速度，但不一定会减少收敛的速度。

## 模型并行（MP）

模型并行可以解决单卡放不下完整模型权重的情况。对模型进行切分，放到不同的卡上去执行。按layer切分模型的话，会有很多bubble，计算资源浪费的很严重。
